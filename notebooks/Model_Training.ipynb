{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing needed Libraries and Packages for Training**"
      ],
      "metadata": {
        "id": "meDiTiStLypT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7bP4VWw3mjYH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "#XGBoost\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "except Exception as e:\n",
        "    raise ImportError(\"Install xgboost: pip install xgboost\") from e\n",
        "\n",
        "#SHAP\n",
        "try:\n",
        "    import shap\n",
        "except Exception:\n",
        "    shap = None\n",
        "    print(\"shap not installed. Install with: pip install shap to get SHAP plots\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "CLEAN_CSV = \"/content/drive/MyDrive/Crop_Prediction_Project/Cleaned_dataset.csv\"  # path to cleaned dataset\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Crop_Prediction_Project\"; os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7127ce"
      },
      "source": [
        "### 1. Load data\n",
        "\n",
        "This step loads the cleaned dataset from the specified CSV file path into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CLEAN_CSV)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwqOljhinoXg",
        "outputId": "b2f2396a-c924-46b7-a7a5-28d0e151a1a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded rows: 48733\n",
            "['Crop', 'Season', 'Soil_Type', 'N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'Yield_t_ha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ff7d2d"
      },
      "source": [
        "**Observation:** The dataset has been loaded successfully, and the number of rows and column names are printed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "120fab99"
      },
      "source": [
        "### 2. Prepare features & target\n",
        "\n",
        "In this step, the features (independent variables) and the target variable (dependent variable) are separated. Categorical and numerical columns are identified, and the data is split into training and testing sets. The splitting is stratified by the 'Crop' column to maintain the distribution of crops in both sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 2. Prepare features & target ----------\n",
        "target_col = \"Yield_t_ha\"\n",
        "cat_cols = [\"Crop\", \"Season\", \"Soil_Type\"]\n",
        "num_cols = [c for c in df.columns if c not in cat_cols + [target_col]]\n",
        "\n",
        "X = df[cat_cols + num_cols].copy()\n",
        "y = df[target_col].copy()\n",
        "\n",
        "# Train-test split (stratify by Crop to keep crop distribution similar)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=X['Crop']\n",
        ")"
      ],
      "metadata": {
        "id": "P6KlcbJ5noPS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36824b3f"
      },
      "source": [
        "**Observation:** The features (X) and target (y) are prepared, and the data is split into training and testing sets. The shapes of the resulting arrays are not explicitly printed, but the process is completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51315cfd"
      },
      "source": [
        "### 3. Preprocessing pipelines\n",
        "\n",
        "This step defines two preprocessing pipelines: one for tree-based models (like Random Forest and XGBoost) and one for linear models (like Linear Regression). The tree-based pipeline uses One-Hot Encoding for categorical features and passes numerical features through. The linear pipeline uses One-Hot Encoding and StandardScaler for numerical features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 3. Preprocessing pipelines ----------\n",
        "# For tree models we'll one-hot encode categorical variables and pass numeric as-is\n",
        "tree_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", \"passthrough\", num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "# For linear regression we scale numeric features and OHE the categoricals\n",
        "linear_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"scale\", StandardScaler(), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")"
      ],
      "metadata": {
        "id": "WbAUgwFHnuA4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59213aab"
      },
      "source": [
        "**Observation:** Two `ColumnTransformer` objects, `tree_preprocessor` and `linear_preprocessor`, are created and configured for different model types. These are ready to be used in the model pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8fec53"
      },
      "source": [
        "### 4. Define models & pipelines\n",
        "\n",
        "This step defines the machine learning models to be used and creates pipelines that combine the preprocessing steps with the models. Pipelines are created for Linear Regression, Random Forest, and XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 4. Define models & pipelines ----------\n",
        "models = {}\n",
        "\n",
        "# Linear Regression pipeline\n",
        "lin_pipe = Pipeline([(\"pre\", linear_preprocessor), (\"lin\", LinearRegression())])\n",
        "models[\"LinearRegression\"] = lin_pipe\n",
        "\n",
        "# Random Forest pipeline\n",
        "rf_pipe = Pipeline([(\"pre\", tree_preprocessor), (\"rf\", RandomForestRegressor(\n",
        "    n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE\n",
        "))])\n",
        "models[\"RandomForest\"] = rf_pipe\n",
        "\n",
        "# XGBoost pipeline\n",
        "xgb_pipe = Pipeline([(\"pre\", tree_preprocessor), (\"xgb\", XGBRegressor(\n",
        "    n_estimators=1000, learning_rate=0.05, max_depth=8,\n",
        "    objective=\"reg:squarederror\", random_state=RANDOM_STATE, n_jobs=-1\n",
        "))])\n",
        "models[\"XGBoost\"] = xgb_pipe\n",
        "\n",
        "# Decision Tree Regressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dt_pipe = Pipeline([(\"pre\", tree_preprocessor), (\"dt\", DecisionTreeRegressor(random_state=RANDOM_STATE))])\n",
        "models[\"DecisionTree\"] = dt_pipe\n",
        "\n",
        "# Support Vector Regressor (Linear)\n",
        "from sklearn.svm import LinearSVR\n",
        "# Need to scale numerical features for SVR\n",
        "svr_pipe = Pipeline([(\"pre\", linear_preprocessor), (\"svr\", LinearSVR(random_state=RANDOM_STATE, max_iter=10000))])\n",
        "models[\"LinearSVR\"] = svr_pipe\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr_pipe = Pipeline([(\"pre\", tree_preprocessor), (\"gbr\", GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=RANDOM_STATE))])\n",
        "models[\"GradientBoosting\"] = gbr_pipe"
      ],
      "metadata": {
        "id": "sE3cqAyant7Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20505470"
      },
      "source": [
        "**Observation:** Three pipelines (`lin_pipe`, `rf_pipe`, and `xgb_pipe`) are defined, each incorporating the appropriate preprocessor and model. These pipelines are stored in the `models` dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cc24879"
      },
      "source": [
        "### 5. Train & evaluate\n",
        "\n",
        "This step iterates through the defined models, trains each pipeline on the training data (`X_train`, `y_train`), and evaluates their performance on the test data (`X_test`, `y_test`). Evaluation metrics calculated include R-squared (R2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). The results are stored in a DataFrame and saved to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 5. Train & evaluate ----------\n",
        "results = []\n",
        "fitted_models = {}\n",
        "\n",
        "for name, pipe in models.items():\n",
        "    print(f\"\\nTraining {name} ...\")\n",
        "    pipe.fit(X_train, y_train)\n",
        "    pred = pipe.predict(X_test)\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    results.append({\"model\": name, \"r2\": r2, \"mae\": mae, \"rmse\": rmse})\n",
        "    fitted_models[name] = pipe\n",
        "    print(f\"{name} => R2: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics_df = pd.DataFrame(results).sort_values(\"r2\", ascending=False)\n",
        "metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"model_metrics.csv\"), index=False)\n",
        "print(\"\\nSaved metrics to:\", os.path.join(OUTPUT_DIR, \"model_metrics.csv\"))\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA7WibM4ntxC",
        "outputId": "2be1312d-3b2b-4287-d896-9fe5c4d7a419"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LinearRegression ...\n",
            "LinearRegression => R2: 0.8153 | MAE: 1.4453 | RMSE: 2.2337\n",
            "\n",
            "Training RandomForest ...\n",
            "RandomForest => R2: 0.8018 | MAE: 1.4884 | RMSE: 2.3136\n",
            "\n",
            "Training XGBoost ...\n",
            "XGBoost => R2: 0.7858 | MAE: 1.4886 | RMSE: 2.4055\n",
            "\n",
            "Training DecisionTree ...\n",
            "DecisionTree => R2: 0.7659 | MAE: 1.5124 | RMSE: 2.5145\n",
            "\n",
            "Training LinearSVR ...\n",
            "LinearSVR => R2: 0.6938 | MAE: 1.4833 | RMSE: 2.8759\n",
            "\n",
            "Training GradientBoosting ...\n",
            "GradientBoosting => R2: 0.8029 | MAE: 1.4657 | RMSE: 2.3075\n",
            "\n",
            "Saved metrics to: /content/drive/MyDrive/Crop_Prediction_Project/model_metrics.csv\n",
            "              model        r2       mae      rmse\n",
            "0  LinearRegression  0.815274  1.445271  2.233668\n",
            "5  GradientBoosting  0.802856  1.465697  2.307526\n",
            "1      RandomForest  0.801818  1.488359  2.313589\n",
            "2           XGBoost  0.785767  1.488581  2.405458\n",
            "3      DecisionTree  0.765906  1.512419  2.514488\n",
            "4         LinearSVR  0.693786  1.483336  2.875856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1aae56e"
      },
      "source": [
        "**Observation:** Each model is trained and evaluated. The R2, MAE, and RMSE scores for all the models are printed to the console and saved to `model_metrics.csv`. The output shows the performance of Linear Regression, Random Forest, XGBoost, Decision Tree, Linear SVR, and Gradient Boosting, with Linear Regression currently being the best-performing model based on R2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c14b9320"
      },
      "source": [
        "### 6. Interpret XGBoost model (SHAP)\n",
        "\n",
        "This step uses the SHAP (SHapley Additive exPlanations) library to interpret the XGBoost model. It samples a subset of the test data, transforms it using the tree preprocessor, and calculates SHAP values. A summary plot and dependence plots for the top features are generated and saved as PNG files. The code includes a fallback mechanism if the preferred `TreeExplainer` fails."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust SHAP block that handles sparse/dense transform outputs\n",
        "import os, numpy as np, matplotlib.pyplot as plt, scipy.sparse as sp\n",
        "import shap\n",
        "\n",
        "OUT = OUTPUT_DIR  # reuse your OUTPUT_DIR\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# 1) Sample for SHAP (smaller to be safe)\n",
        "sample_n = min(500, X_test.shape[0])   # 500 is safer on limited RAM; increase if you have memory\n",
        "X_test_sample = X_test.sample(n=sample_n, random_state=42)\n",
        "\n",
        "# Preprocessor & model from your fitted pipeline\n",
        "pre = fitted_models[\"XGBoost\"].named_steps[\"pre\"]\n",
        "model_xgb = fitted_models[\"XGBoost\"].named_steps[\"xgb\"]\n",
        "\n",
        "# 2) Transform test sample using preprocessor\n",
        "X_test_trans = pre.transform(X_test_sample)\n",
        "\n",
        "# If transform returned a scipy sparse matrix, convert to dense safely\n",
        "if sp.issparse(X_test_trans):\n",
        "    print(\"Detected sparse matrix; converting to dense with toarray() (may use memory).\")\n",
        "    X_test_trans = X_test_trans.toarray()\n",
        "else:\n",
        "    # If it's an object array or nested arrays, try converting safely\n",
        "    X_test_trans = np.asarray(X_test_trans)\n",
        "\n",
        "# Ensure numeric dtype\n",
        "X_test_trans = X_test_trans.astype(np.float32)\n",
        "\n",
        "# 3) Build feature names matching transformed columns\n",
        "cat_cols = [\"Crop\", \"Season\", \"Soil_Type\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "try:\n",
        "    ohe = pre.named_transformers_[\"ohe\"]\n",
        "    # sklearn >=1.0\n",
        "    ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
        "except Exception:\n",
        "    try:\n",
        "        ohe_names = list(ohe.get_feature_names(cat_cols))\n",
        "    except Exception:\n",
        "        # Last resort: combine cat name + unique values (may not match ordering exactly)\n",
        "        ohe_names = []\n",
        "        for c in cat_cols:\n",
        "            vals = list(X[c].astype(str).unique())\n",
        "            vals = [str(v).replace(\" \", \"_\") for v in vals]\n",
        "            ohe_names += [f\"{c}_{v}\" for v in vals]\n",
        "\n",
        "feature_names = ohe_names + num_cols\n",
        "\n",
        "# Sanity: check shapes\n",
        "if X_test_trans.shape[1] != len(feature_names):\n",
        "    print(\"WARNING: feature name count mismatch ->\", X_test_trans.shape[1], \"vs\", len(feature_names))\n",
        "    # Try to proceed but SHAP may still complain. You can inspect feature_names or regenerate them.\n",
        "\n",
        "# 4) Try TreeExplainer with Booster (preferred).\n",
        "saved = False\n",
        "try:\n",
        "    booster = model_xgb.get_booster()\n",
        "    explainer = shap.TreeExplainer(booster)\n",
        "    shap_vals = explainer.shap_values(X_test_trans)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    shap.summary_plot(shap_vals, X_test_trans, feature_names=feature_names, show=False)\n",
        "    plt.tight_layout()\n",
        "    outp = os.path.join(OUT, \"xgb_shap_summary_booster.png\")\n",
        "    plt.savefig(outp); plt.close()\n",
        "    print(\"Saved SHAP summary (booster) to\", outp)\n",
        "    saved = True\n",
        "except Exception as e:\n",
        "    print(\"TreeExplainer(booster) failed:\", repr(e))\n",
        "\n",
        "# 5) Fallback: shap.Explainer using a predict wrapper (robust)\n",
        "if not saved:\n",
        "    print(\"Falling back to shap.Explainer using model.predict (robust fallback).\")\n",
        "    # Build small transformed train sample for masker\n",
        "    train_sample_n = min(300, X_train.shape[0])\n",
        "    X_train_sample = X_train.sample(n=train_sample_n, random_state=42)\n",
        "    X_train_trans = pre.transform(X_train_sample)\n",
        "    if sp.issparse(X_train_trans):\n",
        "        X_train_trans = X_train_trans.toarray()\n",
        "    X_train_trans = np.asarray(X_train_trans).astype(np.float32)\n",
        "\n",
        "    # Create predict function that accepts transformed arrays\n",
        "    def xgb_predict_transformed(mat):\n",
        "        mat = np.asarray(mat).astype(np.float32)\n",
        "        return model_xgb.predict(mat)\n",
        "\n",
        "    # Create an Independent masker on the transformed train data\n",
        "    try:\n",
        "        masker = shap.maskers.Independent(X_train_trans)\n",
        "    except Exception:\n",
        "        masker = shap.maskers.Independent(X_train_trans.astype(np.float32))\n",
        "\n",
        "    # Initialize explainer with the predict callable\n",
        "    explainer = shap.Explainer(xgb_predict_transformed, masker=masker, feature_names=feature_names)\n",
        "    shap_res = explainer(X_test_trans)  # returns Explanation object\n",
        "    # Save summary plot\n",
        "    plt.figure(figsize=(10,6))\n",
        "    shap.summary_plot(shap_res.values, X_test_trans, feature_names=feature_names, show=False)\n",
        "    plt.tight_layout()\n",
        "    outp = os.path.join(OUT, \"xgb_shap_summary_explainer.png\")\n",
        "    plt.savefig(outp); plt.close()\n",
        "    print(\"Saved SHAP summary (Explainer) to\", outp)\n",
        "\n",
        "# 6) Dependence plots for top features (if SHAP succeeded)\n",
        "try:\n",
        "    if 'shap_vals' in locals():\n",
        "        S = np.abs(shap_vals).mean(axis=0)\n",
        "    else:\n",
        "        S = np.abs(shap_res.values).mean(axis=0)\n",
        "    top_idx = np.argsort(S)[-4:][::-1]\n",
        "    for i in top_idx:\n",
        "        fname = feature_names[i] if i < len(feature_names) else f\"f{i}\"\n",
        "        plt.figure(figsize=(6,4))\n",
        "        try:\n",
        "            shap.dependence_plot(i, shap_vals if 'shap_vals' in locals() else shap_res.values,\n",
        "                                 X_test_trans, feature_names=feature_names, show=False)\n",
        "        except Exception:\n",
        "            # fallback to name-based dependence\n",
        "            shap.dependence_plot(fname, shap_vals if 'shap_vals' in locals() else shap_res.values,\n",
        "                                 X_test_trans, feature_names=feature_names, show=False)\n",
        "        plt.tight_layout()\n",
        "        outp = os.path.join(OUT, f\"xgb_shap_dependence_{fname.replace(' ', '_')}.png\")\n",
        "        plt.savefig(outp); plt.close()\n",
        "        print(\"Saved dependence for\", fname, \"->\", outp)\n",
        "except Exception as e_dep:\n",
        "    print(\"Could not create dependence plots:\", repr(e_dep))\n",
        "\n",
        "print(\"Done. Check saved files in:\", OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "_O8daJ7Jn_TS",
        "outputId": "84dd61ca-1396-4d8b-8af9-7bb04c0a23c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected sparse matrix; converting to dense with toarray() (may use memory).\n",
            "TreeExplainer(booster) failed: ValueError(\"could not convert string to float: '[5.3128877E0]'\")\n",
            "Falling back to shap.Explainer using model.predict (robust fallback).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PermutationExplainer explainer: 501it [05:01,  1.63it/s]\n",
            "/tmp/ipython-input-279426551.py:99: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  shap.summary_plot(shap_res.values, X_test_trans, feature_names=feature_names, show=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SHAP summary (Explainer) to /content/drive/MyDrive/Crop_Prediction_Project/xgb_shap_summary_explainer.png\n",
            "Saved dependence for temperature -> /content/drive/MyDrive/Crop_Prediction_Project/xgb_shap_dependence_temperature.png\n",
            "Saved dependence for N -> /content/drive/MyDrive/Crop_Prediction_Project/xgb_shap_dependence_N.png\n",
            "Saved dependence for ph -> /content/drive/MyDrive/Crop_Prediction_Project/xgb_shap_dependence_ph.png\n",
            "Saved dependence for P -> /content/drive/MyDrive/Crop_Prediction_Project/xgb_shap_dependence_P.png\n",
            "Done. Check saved files in: /content/drive/MyDrive/Crop_Prediction_Project\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5487582f"
      },
      "source": [
        "**Observation:** The SHAP analysis for the XGBoost model is performed. The code successfully falls back to using `shap.Explainer` with a predict wrapper. A SHAP summary plot and dependence plots for the top 4 features (temperature, N, ph, and P) are saved as PNG files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7148e11d"
      },
      "source": [
        "### 7. Interpret Linear Regression (coefficients)\n",
        "\n",
        "This step interprets the Linear Regression model by examining the coefficients assigned to each feature. It retrieves the feature names from the linear preprocessor and extracts the coefficients from the trained linear model. The coefficients are stored in a DataFrame, sorted by their absolute values in descending order, and saved to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = fitted_models[\"LinearRegression\"]\n",
        "# get feature names for the linear preprocessor\n",
        "pre_lin = lin.named_steps[\"pre\"]\n",
        "try:\n",
        "    lin_ohe_names = pre_lin.named_transformers_[\"ohe\"].get_feature_names_out(cat_cols)\n",
        "except Exception:\n",
        "    lin_ohe_names = pre_lin.named_transformers_[\"ohe\"].get_feature_names(cat_cols)\n",
        "lin_feature_names = list(lin_ohe_names) + num_cols\n",
        "\n",
        "coeffs = lin.named_steps[\"lin\"].coef_\n",
        "coef_df = pd.DataFrame({\"feature\": lin_feature_names, \"coef\": coeffs})\n",
        "coef_df = coef_df.reindex(coef_df.coef.abs().sort_values(ascending=False).index)\n",
        "coef_df.to_csv(os.path.join(OUTPUT_DIR, \"linear_coefficients.csv\"), index=False)\n",
        "print(\"Saved linear coefficients to\", os.path.join(OUTPUT_DIR, \"linear_coefficients.csv\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peszAsR7n_LP",
        "outputId": "c58f319c-686e-4cea-b118-aa8dec91ce44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved linear coefficients to /content/drive/MyDrive/Crop_Prediction_Project/linear_coefficients.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83fd0d15"
      },
      "source": [
        "**Observation:** The coefficients for the Linear Regression model are calculated and saved to `linear_coefficients.csv`. The output indicates that the file has been successfully saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "751852cc"
      },
      "source": [
        "### 8. Select best-performing model and save\n",
        "\n",
        "This step selects the best-performing model based on the R2 score from the evaluation results. The selected model pipeline is then saved to a joblib file. A summary of the model selection process, including the metrics and the path to the saved model, is also saved to a text file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selection criteria: highest R2 (you can change to lowest RMSE if you prefer)\n",
        "best_row = metrics_df.iloc[0]\n",
        "best_model_name = best_row['model']\n",
        "best_model_pipe = fitted_models[best_model_name]\n",
        "best_model_path = os.path.join(OUTPUT_DIR, f\"best_model_{best_model_name}.joblib\")\n",
        "joblib.dump(best_model_pipe, best_model_path)\n",
        "print(f\"Best model: {best_model_name} saved to {best_model_path}\")\n",
        "\n",
        "# Save a human readable summary\n",
        "with open(os.path.join(OUTPUT_DIR, \"selection_summary.txt\"), \"w\") as f:\n",
        "    f.write(\"Model comparison (sorted by R2):\\n\")\n",
        "    f.write(metrics_df.to_string(index=False))\n",
        "    f.write(\"\\n\\nSelected model: \" + best_model_name + \"\\n\")\n",
        "    f.write(\"Saved model path: \" + best_model_path + \"\\n\")\n",
        "print(\"Saved selection summary to:\", os.path.join(OUTPUT_DIR, \"selection_summary.txt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13YBQMj8n_F4",
        "outputId": "dd3b1b1a-a5ba-4698-8c54-b530203de0fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: LinearRegression saved to /content/drive/MyDrive/Crop_Prediction_Project/best_model_LinearRegression.joblib\n",
            "Saved selection summary to: /content/drive/MyDrive/Crop_Prediction_Project/selection_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d34decb0"
      },
      "source": [
        "**Observation:** The Linear Regression model is identified as the best-performing model based on the R2 score. The trained Linear Regression pipeline is saved as `best_model_LinearRegression.joblib`, and a selection summary is saved to `selection_summary.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bf2c97"
      },
      "source": [
        "### 9. Test the best model with a sample input\n",
        "\n",
        "This step demonstrates how to use the selected best model (Linear Regression) to predict the yield for a sample input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b403dd75",
        "outputId": "44986765-45b7-481e-ea4a-54865e04c7d1"
      },
      "source": [
        "# --------- 9. Test the best model with a sample input ----------\n",
        "\n",
        "# Define a sample input based on the dataset's features\n",
        "# Replace with your desired values for prediction\n",
        "# sample_input = {\n",
        "#     'Crop': 'rice',\n",
        "#     'Season': 'Summer',\n",
        "#     'Soil_Type': 'loamy',\n",
        "#     'N': 80.0,\n",
        "#     'P': 50.0,\n",
        "#     'K': 30.0,\n",
        "#     'temperature': 25.0,\n",
        "#     'humidity': 80.0,\n",
        "#     'ph': 6.0,\n",
        "#     'rainfall': 200.0\n",
        "# }\n",
        "\n",
        "# Convert the sample input to a DataFrame\n",
        "# sample_df = pd.DataFrame([sample_input])\n",
        "\n",
        "# Get the best fitted model\n",
        "best_model_pipe = fitted_models[best_model_name]\n",
        "\n",
        "# Select a sample from the test set\n",
        "sample_index = 0 # You can change this index to pick a different sample\n",
        "sample_input_from_test = X_test.iloc[[sample_index]]\n",
        "actual_yield = y_test.iloc[sample_index]\n",
        "\n",
        "\n",
        "# Make a prediction using the best model\n",
        "predicted_yield = best_model_pipe.predict(sample_input_from_test)\n",
        "\n",
        "print(f\"Sample Input (from test set):\")\n",
        "print(sample_input_from_test)\n",
        "print(f\"\\nActual Yield: {actual_yield:.4f} t/ha\")\n",
        "print(f\"Predicted Yield: {predicted_yield[0]:.4f} t/ha\")\n",
        "\n",
        "# To calculate accuracy, you would need an actual yield value for this sample input.\n",
        "# You can then compare the predicted_yield to the actual yield.\n",
        "# Example:\n",
        "# accuracy = 1 - abs(predicted_yield[0] - actual_yield) / actual_yield\n",
        "# print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Input (from test set):\n",
            "          Crop Season Soil_Type     N     P     K  temperature   humidity  \\\n",
            "9902  chickpea   Rabi     loamy  32.0  60.0  83.0    19.691417  19.442254   \n",
            "\n",
            "            ph   rainfall  \n",
            "9902  8.829273  91.760716  \n",
            "\n",
            "Actual Yield: 5.1667 t/ha\n",
            "Predicted Yield: 4.6975 t/ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95a460b"
      },
      "source": [
        "**Observation:** A sample input from the test set was used to test the best model (Linear Regression). The actual yield for this sample was 5.1667 t/ha, and the model predicted a yield of 4.6975 t/ha. This provides a direct comparison between the model's prediction and the true value for a specific data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66f50698"
      },
      "source": [
        "To further improve the best model (Linear Regression), you can consider the following:\n",
        "\n",
        "*   **Feature Engineering:** Create new features from existing ones (e.g., interactions, polynomial terms).\n",
        "*   **Handling Categorical Features:** Explore alternative encoding methods beyond one-hot encoding.\n",
        "*   **Regularization:** Apply L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting.\n",
        "*   **Polynomial Features:** Introduce polynomial features to capture non-linear relationships.\n",
        "*   **Outlier Detection and Handling:** Identify and address outliers in the dataset.\n",
        "*   **Hyperparameter Tuning:** Tune the parameters of the preprocessing steps.\n",
        "*   **Alternative Linear Models:** Experiment with other linear models like Ridge, Lasso, or Elastic Net.\n",
        "*   **Ensemble Methods:** Combine predictions from multiple models."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zZmL1RWNtn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}